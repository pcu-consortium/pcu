<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE xml>
<!-- 
   Copyright 2010-2017 Norconex Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<!-- This self-documented configuration file is meant to be used as a reference
     or starting point for a new configuration. 
     It contains all core features offered in this release.  Sometimes
     multiple implementations are available for a given feature. Refer 
     to site documentation for more options and complete description of 
     each features.
     -->
<httpcollector id="My Collector Name">

  <!-- Variables: Optionally define variables in this configuration file
       using the "set" directive, or by using a file of the same name
       but with the extension ".variables" or ".properties".  Refer 
       to site documentation to find out what each extension does.
       Also, one can pass an optional properties file when starting the
       crawler.  The following is good practice to reference frequently 
       used classes in a shorter way.
       -->
  #set($workdir = "c:\path\to\your\workdir")

  #set($core      = "com.norconex.collector.core")
  #set($http      = "com.norconex.collector.http")
  #set($committer = "com.norconex.committer")


  #set($httpClientFactory = "${http}.client.impl.GenericHttpClientFactory")
  #set($filterExtension   = "${core}.filter.impl.ExtensionReferenceFilter")
  #set($filterRegexRef    = "${core}.filter.impl.RegexReferenceFilter")
  #set($filterRegexMeta   = "${core}.filter.impl.RegexMetadataFilter")
  #set($robotsTxt         = "${http}.robot.impl.StandardRobotsTxtProvider")
  #set($robotsMeta        = "${http}.robot.impl.StandardRobotsMetaProvider")
  #set($redirectProvider  = "${http}.redirect.impl.GenericRedirectURLProvider")
  #set($recrawlResolver   = "${http}.recrawl.impl.GenericRecrawlableResolver")
  #set($metaFetcher       = "${http}.fetch.impl.GenericMetadataFetcher")
  #set($docFetcher        = "${http}.fetch.impl.GenericDocumentFetcher")
  #set($linkExtractor     = "${http}.url.impl.GenericLinkExtractor")
  #set($canonLinkDetector = "${http}.url.impl.GenericCanonicalLinkDetector")
  #set($urlNormalizer     = "${http}.url.impl.GenericURLNormalizer")
  #set($delayResolver     = "${http}.delay.impl.GenericDelayResolver")
  #set($sitemapFactory    = "${http}.sitemap.impl.StandardSitemapResolverFactory")
  #set($metaChecksummer   = "${http}.checksum.impl.HttpMetadataChecksummer")
  #set($docChecksummer    = "${core}.checksum.impl.MD5DocumentChecksummer")
  #set($dataStoreFactory  = "${core}.data.store.impl.mvstore.MVStoreCrawlDataStoreFactory")
  #set($spoiledStrategy   = "${core}.spoil.impl.GenericSpoiledReferenceStrategizer")
  #set($fsCommitter       = "${committer}.core.impl.FileSystemCommitter")
  
  
  <!-- Location where internal progress files are stored. -->
  <progressDir>$workdir\progress</progressDir>

  <!-- Location where logs are stored. -->
  <logsDir>$workdir\logs</logsDir>

  <!-- One or more optional listeners to be notified when the collector
       starts or finishes.  Class must implement 
       com.norconex.collector.core.ICollectorLifeCycleListener
       -->
  <collectorListeners>
    <listener class="YourClass"/>
  </collectorListeners>

  <!-- All crawler configuration options can be specified as default 
       (including start URLs).  Settings defined here will be inherited by 
       all individual crawlers defined further down, unless overwritten.
       If you replace a top level crawler tag from the crawler defaults,
       all the default tag configuration settings will be replaced, no 
       attempt will be made to merge or append.
       -->
  <crawlerDefaults>

    <!-- Mandatory starting URL(s) where crawling begins.  If you put more 
         than one URL, they will be processed together.  You can also
         point to one or more URLs files (i.e., seed lists), or
         point to a sitemap.xml. If the list of URLs needs to be dynamically
         built, you can use one or more custom IStartURLsProvider.  -->    
    <startURLs stayOnDomain="false" stayOnPort="false" stayOnProtocol="false">
      <url>http://www.example.com</url>
      <url>http://www.sample.com</url>
      <urlsFile>/local/path/to/a/file/full/of/urls.txt</urlsFile>
      <sitemap>http://www.somewhere.com/sitemap.xml</sitemap>
      <provider class="YourClass"/>
    </startURLs>

    <!-- Identify yourself to sites you crawl.  It sets the "User-Agent" HTTP 
         request header value.  This is how browsers identify themselves for
         instance.  Sometimes required to be certain values for robots.txt 
         files.
      -->
    <userAgent>Please identify your Crawler</userAgent>

    <!-- Optional URL normalization feature. The class must implement
         com.norconex.collector.http.url.IURLNormalizer, 
         like the following class does.
      -->
    <urlNormalizer class="$urlNormalizer">
      <normalizations>
        lowerCaseSchemeHost, upperCaseEscapeSequence, 
        decodeUnreservedCharacters, removeDefaultPort 
      </normalizations>
      <replacements>
        <replace>
           <match>&amp;view=print</match>
           <replacement>&amp;view=html</replacement>
        </replace>
      </replacements>
    </urlNormalizer>

    <!-- Optional delay resolver defining how polite or aggressive you want
         your crawling to be.  The class must implement 
         com.norconex.collector.http.delay.IDelayResolver.
         The following class is the default implementation 
         (but the schedule sample is not):
      -->
    <delay default="1000" ignoreRobotsCrawlDelay="true"
           class="$delayResolver">
          <schedule dayOfWeek="from Monday to Friday" 
                    time="from 8:00 to 16:30">10000</schedule>
    </delay>

    <!-- How many threads you want a crawler to use.  Regardless of how many
         thread you have running, the frequency of each URL being invoked
         will remain dictated by the &lt;delay/&gt option above.  Using more
         than one thread is a good idea to ensure the delay is respected
         in case you run into single downloads taking more time than the
         delay specified. Default is 2 threads.
      -->
    <numThreads>2</numThreads>

    <!-- How many level deep can the crawler go. I.e, within how many clicks 
         away from the main page (start URL) each page can be to be considered.
         Beyond the depth specified, pages are rejected.
         The starting URLs all have a zero-depth.  Default is -1 (unlimited)
         -->
    <maxDepth>5</maxDepth>
    
    <!-- Stop crawling after how many successfully processed documents.  
         A successful document is one that is either new or modified, that was 
         not rejected, not deleted, or did not generate any error.  As an
         example, this is a document that will end up in your search engine. 
         Default is -1 (unlimited)
          -->
    <maxDocuments>-1</maxDocuments>

    <!-- Crawler "work" directory.  This is where files downloaded or created as
         part of crawling activities (besides logs and progress) get stored.
         It should be unique to each crawlers.
         -->
    <workDir>/crawler/workdir/path</workDir>

    <!-- Keep downloaded files. Default is false.
         -->
    <keepDownloads>false</keepDownloads>

    <!-- Keep extracted links that are out-of-scope according to start URL
         stayOn... flags (if any are true).  Out-of-scope links are 
         stored in a metadata field.
         -->
    <keepOutOfScopeLinks>false</keepOutOfScopeLinks>

    <!-- What to do with orphan documents.  Orphans are valid 
         documents, which on subsequent crawls can no longer be reached when 
         running the crawler (e.g. there are no links pointing to that page 
         anymore).  Available options are: 
         IGNORE, DELETE, and PROCESS (default).
         -->
    <orphansStrategy>PROCESS</orphansStrategy>

    <!-- One or more fully qualified names of Java exceptions
         that should force a crawler to stop when triggered during the 
         processing of a document. 
         Default is empty (will try to continue upon exceptions).
         -->
    <stopOnExceptions>
        <exception>com.norconex.committer.core.CommitterException</exception>
    </stopOnExceptions>

    <!-- One or more optional listeners to be notified on various crawling
         events (e.g. document rejected, document imported, etc). 
         Class must implement 
         com.norconex.collector.core.event.ICrawlerEventListener
         -->
    <crawlerListeners>
      <listener class="YourClass"/>
    </crawlerListeners>

    <!-- Factory class creating a database for storing crawl status and
         other information.  Classes must implement 
         com.norconex.collector.core.data.store.ICrawlURLDatabaseFactory.  
         Default implementation is the following.
         -->
    <crawlDataStoreFactory class="$dataStoreFactory" />

    <!-- Initialize the HTTP client use to make connections.  Classes
         must implement com.norconex.collector.http.client.IHttpClientFactory.
         Default implementation offers many options. The following shows
         a sample use of the default with credentials.
         -->
    <httpClientFactory class="$httpClientFactory">
      <!-- These apply to any authentication mechanism -->
      <authUsername>myusername</authUsername>
      <authPassword>mypassword</authPassword>
      <!-- These apply to FORM authentication -->
      <authUsernameField>field_username</authUsernameField>
      <authPasswordField>field_password</authPasswordField>
      <authURL>https://www.example.com/login.php</authURL>
      <!-- These apply to both BASIC and DIGEST authentication -->
      <authHostname>www.example.com</authHostname>
      <authPort>80</authPort>
      <authRealm>PRIVATE</authRealm>
    </httpClientFactory>
    
    <!-- Optionally filter URL BEFORE any download. Classes must implement 
         com.norconex.collector.core.filter.IReferenceFilter, 
         like the following examples.
         -->
    <referenceFilters>
      <filter class="$filterExtension" onMatch="exclude" >
        jpg,gif,png,ico,css,js</filter>
      <filter class="$filterRegexRef">https://www.example.com/.*</filter>
    </referenceFilters>

    <!-- Filter BEFORE download with RobotsTxt rules. Classes must
         implement *.robot.IRobotsTxtProvider.  Default implementation
         is the following.
         -->
    <robotsTxt ignore="false" class="$robotsTxt"/>
    
    <!-- Loads sitemap.xml URLs and adds adds them to URLs to process.
         Default implementation is the following.
         -->
    <sitemapResolverFactory 
          ignore="false" lenient="false" class="$sitemapFactory">
      <path>/blogs/sitemap.xml</path>
    </sitemapResolverFactory>
    
    <!-- Provides the target URL to use when a redirect is encountered.
         Classes must implement *.redirect.IRedirectURLProvider.
         Default implementation is the following.
         -->
    <redirectURLProvider class="$redirectProvider" fallbackCharset="" />
    
    
    <!-- Indicates if a target URL is ready for recrawl or not.
         Default implementation is the following.
         -->
    <recrawlableResolver class="$recrawlResolver" />
    
    <!-- Fetch a URL HTTP Headers.  Classes must implement
         com.norconex.collector.http.fetch.IHttpMetadataFetcher.  
         The following is a simple implementation.
         HTTP headers are normally fetched by the "documentFetcher". Use
         this only when necessary (see documentation).
         -->
    <metadataFetcher class="$metaFetcher" validStatusCodes="200" />

    <!-- Optionally filter AFTER download of HTTP headers.  Classes must 
         implement com.norconex.collector.core.filter.IMetadataFilter.  
         -->
    <metadataFilters>
      <filter class="$filterRegexMeta" 
              onMatch="exclude"
              caseSensitive="false"
              field="Content-Type">.*css.*</filter>
    </metadataFilters>        

    <!-- Detect canonical links. Classes must implement
         com.norconex.collector.http.url.ICanonicalLinkDetector.
         Default implementation is the following.
         -->
    <canonicalLinkDetector ignore="false" class="${canonLinkDetector}">
      <contentTypes>
          text/html, application/xhtml+xml, vnd.wap.xhtml+xml, x-asp
      </contentTypes>
    </canonicalLinkDetector>

    <!-- Generates a checksum value from document headers to find out if 
         a document has changed. Class must implement
         com.norconex.collector.core.checksum.IMetadataChecksummer.  
         Default implementation is the following. 
         -->
    <metadataChecksummer class="$metaChecksummer" />

    <!-- Fetches document.  Class must implement 
         com.norconex.collector.http.fetch.IHttpDocumentFetcher.  
         Default implementation is the following.
         -->
    <documentFetcher class="$docFetcher" validStatusCodes="200" />

    <!-- Establish whether to follow a page URLs or to index a given page
         based on in-page meta tag robot information. Classes must implement 
         com.norconex.collector.http.robot.IRobotsMetaProvider.  
         Default implementation is the following.
         -->
    <robotsMeta ignore="false" class="$robotsMeta" />

    <!-- Extract links from a document.  Classes must implement
         com.norconex.collector.http.url.ILinkExtractor. 
         Default implementation is the following.
         -->
    <linkExtractors>
        <extractor class="${linkExtractor}"  maxURLLength="2048" 
                ignoreNofollow="false" commentsEnabled="false">
            <contentTypes>
                text/html, application/xhtml+xml, vnd.wap.xhtml+xml, x-asp
            </contentTypes>
            <tags>
                <tag name="a" attribute="href" />
                <tag name="frame" attribute="src" />
                <tag name="iframe" attribute="src" />
                <tag name="img" attribute="src" />
                <tag name="meta" attribute="http-equiv" />
            </tags>
        </extractor>
    </linkExtractors>

    <!--  Optionally filters a document. Classes must implement 
          com.norconex.collector.core.filter.IDocumentFilter-->
    <documentFilters>
        <filter class="YourClass" />
    </documentFilters>

    <!-- Optionally process a document BEFORE importing it. Classes must
         implement com.norconex.collector.http.doc.IHttpDocumentProcessor.
         -->
    <preImportProcessors>
        <processor class="YourClass"></processor>
    </preImportProcessors>
      
    <!-- Import a document.  This step calls the Importer module.  The
         importer is a different module with its own set of XML configuration
         options.  Please refer to Importer for complete documentation.
         Below gives you an overview of the main importer tags.
         -->
    <importer>
        <preParseHandlers>
            <tagger class="..."/>
            <transformer class="..." />
            <filter class="..." />
            <splitter class="..." />
        </preParseHandlers>
        <documentParserFactory class="..." />
        <postParseHandlers>
            <tagger class="..."/>
            <transformer class="..." />
            <filter class="..." />
            <splitter class="..." />
        </postParseHandlers>
        <responseProcessors>
            <responseProcessor class="YourClass" />
        </responseProcessors>
    </importer>           


    <!-- Create a checksum out of a document to figure out if a document
         has changed, AFTER it has been imported. Class must implement 
         com.norconex.collector.core.checksum.IDocumentChecksummer.
         Default implementation is the following.
         -->
    <documentChecksummer class="$docChecksummer" />

    <!-- Optionally process a document AFTER importing it. Classes must
         implement com.norconex.collector.http.doc.IHttpDocumentProcessor.
         -->
    <postImportProcessors>
        <processor class="YourClass"></processor>
    </postImportProcessors>
      
    <!-- Decide what to do with references that have turned bad.
         Class must implement 
         com.norconex.collector.core.spoil.ISpoiledReferenceStrategizer.
         Default implementation is the following.
         -->
    <spoiledReferenceStrategizer class="$spoiledStrategy"
            fallbackStrategy="DELETE">
        <mapping state="NOT_FOUND"  strategy="DELETE" />
        <mapping state="BAD_STATUS" strategy="GRACE_ONCE" />
        <mapping state="ERROR"      strategy="GRACE_ONCE" />
    </spoiledReferenceStrategizer>
      

    <!-- Commits a document to a data source of your choice.
         This step calls the Committer module.  The
         committer is a different module with its own set of XML configuration
         options.  Please refer to committer for complete documentation.
         Below is an example using the FileSystemCommitter.
         -->
    <committer class="$fsCommitter">
      <directory>$workdir\crawledFiles</directory>
    </committer>

  </crawlerDefaults>


  <!-- Individual crawlers can be defined here.  All crawler default
       configuration settings will apply to all crawlers created unless 
       explicitly overwritten in crawler configuration.
       For configuration options where multiple items can be present 
       (e.g. filters), the whole list will in crawler defaults would be
       overwritten.
       Since the options are the same as the defaults above, the documentation 
       is not repeated here.
       The only difference from "crawlerDefaults" is the addition of the "id"
       attribute on the crawler tag.  The "id" attribute uniquely identifies
       each of your crawlers.  
       -->
  <crawlers>
    <crawler id="My Crawler Name">
       <!-- Overwrite any defaults here. -->
    </crawler>
  </crawlers>


  <!-- === FOR ADVANCED USE ONLY ===============================================
       The following listeners are made to work with the JEF API
       (https://www.norconex.com/jef/api/). Usage is recommended only 
       to programmers familiar with the JEF API.
       Most users should ignore these.   -->
       
  <!-- Listen for JEF job events. The class must implement
         com.norconex.jef4.job.IJobLifeCycleListener  -->
  <jobLifeCycleListeners>
      <listener class="YourClass" />
  </jobLifeCycleListeners>

  <!-- Listen for JEF errors. The class must implement
         com.norconex.jef4.job.IJobErrorListener  -->
  <jobErrorListeners>
      <listener class="YourClass" />
  </jobErrorListeners>

  <!-- Listen for JEF job events. The class must implement
         com.norconex.jef4.suite.ISuiteLifeCycleListener  -->
  <suiteLifeCycleListeners>
      <listener class="YourClass" />
  </suiteLifeCycleListeners>

</httpcollector>